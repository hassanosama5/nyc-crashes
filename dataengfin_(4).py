# -*- coding: utf-8 -*-
"""DATAENGFIN_(4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VyjduYPM6lPoa8zIUBhl-QHLXkWsD375
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

plt.style.use("default")
sns.set_theme(style="whitegrid")

import pandas as pd

# -------------------------------
# LOAD CRASHES FIRST (CHUNKS)
# -------------------------------
crashes_url = "https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=download"
crashes_iter = pd.read_csv(crashes_url, low_memory=False, chunksize=250000)

# Load 3 chunks (750k crashes)
df_crashes = pd.concat([next(crashes_iter) for _ in range(3)], ignore_index=True)

# -------------------------------
# SAMPLE CRASHES (the driver)
# -------------------------------
df_crashes = df_crashes.sample(300000, random_state=42).reset_index(drop=True)

# Store sampled IDs
sampled_ids = df_crashes['COLLISION_ID'].dropna().unique()

print("Raw crash sample:", df_crashes.shape)
print("Unique sampled IDs:", len(sampled_ids))

# -------------------------------
# LOAD VEHICLES IN CHUNKS + FILTER
# -------------------------------
vehicles_url = "https://data.cityofnewyork.us/api/views/bm4k-52h4/rows.csv?accessType=download"
vehicles_iter = pd.read_csv(vehicles_url, low_memory=False, chunksize=250000)

vehicles_list = []

for chunk in vehicles_iter:
    vehicles_list.append(chunk[chunk['COLLISION_ID'].isin(sampled_ids)])

df_vehicles = pd.concat(vehicles_list, ignore_index=True)

print("Raw vehicles matched:", df_vehicles.shape)

df_crashes.shape, df_vehicles.shape

print("Crashes dataset:")
df_crashes.head()

print("Vehicles dataset:")
df_vehicles.head()

df_crashes.isna().sum().sort_values(ascending=False).head(15)

df_vehicles.isna().sum().sort_values(ascending=False).head(15)

# ================================
# PRE-INTEGRATION EDA â€” CRASHES
# ================================
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

plt.style.use("default")
sns.set_theme(style="whitegrid")

# ======================================
# 1. Missing Values Heatmap (Crashes)
# ======================================
plt.figure(figsize=(14,6))
sns.heatmap(df_crashes.isna(), cbar=False)
plt.title("Missing Values Heatmap â€” Crashes Dataset")
plt.show()

# ======================================
# 2. Crashes Per Year
# ======================================
df_crashes['CRASH DATE'] = pd.to_datetime(df_crashes['CRASH DATE'], errors='coerce')
df_crashes['YEAR'] = df_crashes['CRASH DATE'].dt.year

year_counts = df_crashes['YEAR'].value_counts().sort_index()

plt.figure(figsize=(10,5))
sns.lineplot(x=year_counts.index, y=year_counts.values, marker="o")
plt.title("Crashes Per Year (Trend)")
plt.xlabel("Year")
plt.ylabel("Number of Crashes")
plt.show()

# ======================================
# 3. Crashes per Month (Seasonality)
# ======================================
df_crashes['MONTH'] = df_crashes['CRASH DATE'].dt.month

plt.figure(figsize=(10,5))
sns.countplot(x='MONTH', data=df_crashes, palette="viridis")
plt.title("Monthly Crash Distribution")
plt.xlabel("Month")
plt.ylabel("Crash Count")
plt.show()

# ======================================
# 4. Crashes by Borough
# ======================================
plt.figure(figsize=(8,5))
sns.countplot(y='BOROUGH', data=df_crashes)
plt.title("Crashes by Borough")
plt.xlabel("Count")
plt.ylabel("Borough")
plt.show()

# ======================================
# 5. Hourly Crash Pattern
# ======================================
df_crashes['HOUR'] = pd.to_datetime(df_crashes['CRASH TIME'], errors='coerce').dt.hour

plt.figure(figsize=(10,5))
sns.histplot(df_crashes['HOUR'], bins=24)
plt.title("Crash Distribution by Hour of the Day")
plt.xlabel("Hour")
plt.ylabel("Count")
plt.show()

# ======================================
# 6. Heatmap â€” Day of Week vs Hour
# ======================================
df_crashes['DAY_OF_WEEK'] = df_crashes['CRASH DATE'].dt.day_name()
pivot = pd.crosstab(df_crashes['DAY_OF_WEEK'], df_crashes['HOUR'])

plt.figure(figsize=(14,6))
sns.heatmap(pivot, cmap="YlOrRd")
plt.title("Heatmap â€” Crashes by Day of Week and Hour")
plt.show()

# ======================================
# 7. Top Contributing Factors
# ======================================
factor_cols = [
    'CONTRIBUTING FACTOR VEHICLE 1',
    'CONTRIBUTING FACTOR VEHICLE 2'
]

factors = pd.concat([df_crashes[col] for col in factor_cols]).dropna()
top_factors = factors.value_counts().head(15)

plt.figure(figsize=(10,6))
sns.barplot(x=top_factors.values, y=top_factors.index)
plt.title("Top 15 Contributing Factors")
plt.xlabel("Count")
plt.ylabel("Factor")
plt.show()

# ======================================
# 8. Crash Severity (Injuries & Deaths)
# ======================================
severity = df_crashes[['NUMBER OF PERSONS INJURED','NUMBER OF PERSONS KILLED']]

plt.figure(figsize=(10,5))
sns.boxplot(data=severity)
plt.title("Injury & Fatality Distribution")
plt.show()

crashes = df_crashes.copy()

# -------------------------------------------------------------------
# 1) SAFE DATE CLEANING (fixes your Tuesday/400-row problem)
# -------------------------------------------------------------------

# Strip whitespace and force everything to string
crashes['CRASH DATE'] = crashes['CRASH DATE'].astype(str).str.strip()

# Replace obvious invalid values BEFORE conversion
bad_values = ['Unknown', 'unknown', 'N/A', 'NaN', 'nan', 'NONE', 'None', '']
crashes['CRASH DATE'] = crashes['CRASH DATE'].replace(bad_values, pd.NA)

# Convert safely WITHOUT coercing everything to NaT
crashes['CRASH DATE'] = pd.to_datetime(crashes['CRASH DATE'], format='mixed', errors='coerce')

# Drop only rows STILL NaT (small number, not 99%)
crashes = crashes[crashes['CRASH DATE'].notna()].copy()

# -------------------------------------------------------------------
# 2) SAFE TIME CLEANING
# -------------------------------------------------------------------

crashes['CRASH TIME'] = crashes['CRASH TIME'].astype(str).str.strip()

# Normalize to HH:MM format
crashes['CRASH TIME'] = crashes['CRASH TIME'].str.extract(r'(\d{1,2}:\d{2})')

# Any missing time â†’ set to 00:00 (keeps dataset large)
crashes['CRASH TIME'] = crashes['CRASH TIME'].fillna('00:00')

# -------------------------------------------------------------------
# 3) Combine DATE + TIME safely
# -------------------------------------------------------------------

crashes['CRASH_DATETIME'] = pd.to_datetime(
    crashes['CRASH DATE'].dt.strftime("%Y-%m-%d") + " " + crashes['CRASH TIME'],
    errors='coerce'
)

# Drop only the rows where datetime still failed (very few)
crashes = crashes[crashes['CRASH_DATETIME'].notna()].copy()

# -------------------------------------------------------------------
# 4) Extract date components
# -------------------------------------------------------------------

crashes['YEAR'] = crashes['CRASH_DATETIME'].dt.year
crashes['MONTH'] = crashes['CRASH_DATETIME'].dt.month
crashes['HOUR'] = crashes['CRASH_DATETIME'].dt.hour
crashes['DAY_OF_WEEK'] = crashes['CRASH_DATETIME'].dt.day_name()

# -------------------------------------------------------------------
# 5) BOROUGH clean
# -------------------------------------------------------------------

crashes['BOROUGH'] = crashes['BOROUGH'].fillna('Unknown').str.title().str.strip()

# -------------------------------------------------------------------
# 6) ZIP CODE clean
# -------------------------------------------------------------------

crashes['ZIP CODE'] = (
    crashes['ZIP CODE']
    .astype(str)
    .str.extract(r'(\d{5})')  # extract valid 5-digit ZIPs only
    .fillna('00000')
)

# -------------------------------------------------------------------
# 7) LAT / LON
# -------------------------------------------------------------------

crashes['LATITUDE'] = pd.to_numeric(crashes['LATITUDE'], errors='coerce')
crashes['LONGITUDE'] = pd.to_numeric(crashes['LONGITUDE'], errors='coerce')

# -------------------------------------------------------------------
# 8) Street names
# -------------------------------------------------------------------

street_cols = ['ON STREET NAME', 'CROSS STREET NAME', 'OFF STREET NAME']
for col in street_cols:
    crashes[col] = crashes[col].fillna('Unknown').str.title().str.strip()

# -------------------------------------------------------------------
# 9) Contributing factors
# -------------------------------------------------------------------

factor_cols = [
    'CONTRIBUTING FACTOR VEHICLE 1',
    'CONTRIBUTING FACTOR VEHICLE 2',
    'CONTRIBUTING FACTOR VEHICLE 3',
    'CONTRIBUTING FACTOR VEHICLE 4',
    'CONTRIBUTING FACTOR VEHICLE 5'
]

for col in factor_cols:
    crashes[col] = (
        crashes[col]
        .fillna('Unspecified')
        .replace('nan', 'Unspecified')
        .astype(str).str.title().str.strip()
    )

# -------------------------------------------------------------------
# 10) Vehicle type codes
# -------------------------------------------------------------------

vehicle_cols = [
    'VEHICLE TYPE CODE 1',
    'VEHICLE TYPE CODE 2',
    'VEHICLE TYPE CODE 3',
    'VEHICLE TYPE CODE 4',
    'VEHICLE TYPE CODE 5'
]

for col in vehicle_cols:
    crashes[col] = crashes[col].fillna('Unknown').astype(str).str.title().str.strip()

# -------------------------------------------------------------------
# 11) Cap injuries
# -------------------------------------------------------------------

crashes['NUMBER OF PERSONS INJURED'] = crashes['NUMBER OF PERSONS INJURED'].clip(0, 20)
crashes['NUMBER OF PERSONS KILLED'] = crashes['NUMBER OF PERSONS KILLED'].clip(0, 5)

injury_cols = [
    'NUMBER OF PEDESTRIANS INJURED',
    'NUMBER OF PEDESTRIANS KILLED',
    'NUMBER OF CYCLIST INJURED',
    'NUMBER OF CYCLIST KILLED',
    'NUMBER OF MOTORIST INJURED',
    'NUMBER OF MOTORIST KILLED'
]

for col in injury_cols:
    crashes[col] = crashes[col].fillna(0)

# -------------------------------------------------------------------
# 12) Location
# -------------------------------------------------------------------

crashes['LOCATION'] = crashes['LOCATION'].fillna('Unknown')

# -------------------------------------------------------------------
# 13) Remove duplicate collisions
# -------------------------------------------------------------------

crashes = crashes.drop_duplicates(subset='COLLISION_ID', keep='first')

# -------------------------------------------------------------------
# 14) Drop missing collision IDs
# -------------------------------------------------------------------

crashes = crashes.dropna(subset=['COLLISION_ID'])

crashes.info()
crashes.isna().sum().sort_values(ascending=False).head(10)

# ================================
# PRE-INTEGRATION EDA â€” VEHICLES
# ================================

# ======================================
# 1. Vehicle Type Distribution
# ======================================
vehicle_types = df_vehicles['VEHICLE_TYPE'].value_counts().head(15)

plt.figure(figsize=(12,6))
sns.barplot(x=vehicle_types.values, y=vehicle_types.index)
plt.title("Top 15 Vehicle Types")
plt.xlabel("Count")
plt.ylabel("Vehicle Type")
plt.show()

# ======================================
# 2. Driver Sex Distribution
# ======================================
driver_sex = df_vehicles['DRIVER_SEX'].value_counts()

plt.figure(figsize=(6,4))
sns.barplot(x=driver_sex.index, y=driver_sex.values, palette="cool")
plt.title("Driver Sex Distribution")
plt.xlabel("Sex")
plt.ylabel("Count")
plt.show()

# ======================================
# 3. Vehicle Model Year Distribution
# ======================================
plt.figure(figsize=(10,5))
sns.histplot(df_vehicles['VEHICLE_YEAR'], bins=40)
plt.title("Vehicle Model Year Distribution")
plt.xlabel("Year")
plt.ylabel("Count")
plt.show()

# ======================================
# 4. Top Pre-Crash Actions
# ======================================
top_precrash = df_vehicles['PRE_CRASH'].value_counts().head(10)

plt.figure(figsize=(10,5))
sns.barplot(x=top_precrash.values, y=top_precrash.index)
plt.title("Top Pre-Crash Vehicle Actions")
plt.xlabel("Count")
plt.ylabel("Action")
plt.show()

# ======================================
# 5. Travel Direction Distribution
# ======================================
plt.figure(figsize=(10,5))
sns.countplot(y='TRAVEL_DIRECTION', data=df_vehicles, order=df_vehicles['TRAVEL_DIRECTION'].value_counts().index)
plt.title("Vehicle Travel Direction")
plt.show()

vehicles = df_vehicles.copy()

# -------------------------------------------------------------------
# 1) SAFE DATE CLEANING (prevents NaT explosion)
# -------------------------------------------------------------------

vehicles['CRASH_DATE'] = vehicles['CRASH_DATE'].astype(str).str.strip()

bad_values = ['Unknown', 'unknown', 'N/A', 'NaN', 'nan', 'NONE', 'None', '']
vehicles['CRASH_DATE'] = vehicles['CRASH_DATE'].replace(bad_values, pd.NA)

vehicles['CRASH_DATE'] = pd.to_datetime(
    vehicles['CRASH_DATE'],
    format='mixed',
    errors='coerce'
)

vehicles = vehicles[vehicles['CRASH_DATE'].notna()].copy()

# -------------------------------------------------------------------
# 2) SAFE TIME CLEANING
# -------------------------------------------------------------------

vehicles['CRASH_TIME'] = vehicles['CRASH_TIME'].astype(str).str.strip()
vehicles['CRASH_TIME'] = vehicles['CRASH_TIME'].str.extract(r'(\d{1,2}:\d{2})')
vehicles['CRASH_TIME'] = vehicles['CRASH_TIME'].fillna('00:00')

# -------------------------------------------------------------------
# 3) SAFE DATETIME COMBINATION
# -------------------------------------------------------------------

vehicles['CRASH_DATETIME'] = pd.to_datetime(
    vehicles['CRASH_DATE'].dt.strftime('%Y-%m-%d') + " " + vehicles['CRASH_TIME'],
    errors='coerce'
)

vehicles = vehicles[vehicles['CRASH_DATETIME'].notna()].copy()

# -------------------------------------------------------------------
# 4) Extract datetime parts
# -------------------------------------------------------------------

vehicles['YEAR'] = vehicles['CRASH_DATETIME'].dt.year
vehicles['MONTH'] = vehicles['CRASH_DATETIME'].dt.month
vehicles['HOUR'] = vehicles['CRASH_DATETIME'].dt.hour
vehicles['DAY_OF_WEEK'] = vehicles['CRASH_DATETIME'].dt.day_name()

# -------------------------------------------------------------------
# 5) Clean basic text columns: MAKE & MODEL
#     - missing -> 'Unknown'
#     - 'nan' / '' -> 'Unknown'
# -------------------------------------------------------------------

text_cols = ['VEHICLE_MAKE', 'VEHICLE_MODEL']

for col in text_cols:
    vehicles[col] = (
        vehicles[col]
        .fillna('Unknown')
        .astype(str)
        .str.strip()
    )
    vehicles[col] = vehicles[col].replace(
        ['nan', 'NaN', 'NONE', 'None', '', 'N/A', 'n/a'],
        'Unknown'
    )
    vehicles[col] = vehicles[col].str.title()

# -------------------------------------------------------------------
# 6) CLEAN VEHICLE_TYPE (IMPORTANT FOR WEBSITE)
#     - normalize, remove garbage
#     - missing-like -> 'Unknown'
#     - non-logical / rare -> 'Other'
# -------------------------------------------------------------------

# 6.1 normalize to lowercase, strip spaces
vehicles['VEHICLE_TYPE'] = (
    vehicles['VEHICLE_TYPE']
    .astype(str)
    .str.strip()
    .str.lower()
)

# 6.2 handle missing-like strings as 'unknown'
missing_type_values = [
    'nan', 'none', 'null', '', 'unspecified', 'n/a', '-', '--', 'unknown'
]
vehicles['VEHICLE_TYPE'] = vehicles['VEHICLE_TYPE'].replace(
    missing_type_values,
    'unknown'
)

# 6.3 numeric-only -> non logical -> 'other'
vehicles['VEHICLE_TYPE'] = vehicles['VEHICLE_TYPE'].apply(
    lambda x: 'other' if x.replace('.', '').isdigit() else x
)

# 6.4 very short junk (< 2 chars) -> 'other'
vehicles['VEHICLE_TYPE'] = vehicles['VEHICLE_TYPE'].apply(
    lambda x: 'other' if len(str(x)) < 2 else x
)

# 6.5 fix common abbreviations / typos / weird strings
vehicle_mapping = {
    'sdn': 'sedan',
    '4ds': 'sedan',
    '4dsd': 'sedan',
    '4 dr sedan': 'sedan',
    'sedn': 'sedan',
    'suden': 'sedan',

    'suv': 'sport utility vehicle',
    'station wagon/sport utility vehicle': 'sport utility vehicle',

    'yello': 'yellow cab',
    'yellowpowe': 'yellow cab',
    'yellow tax': 'yellow cab',

    'mcycle': 'motorcycle',
    'motorcyle': 'motorcycle',
    'motocycle': 'motorcycle',

    'pick-up truck': 'pickup truck',
    'pickup': 'pickup truck',

    '50cc scoot': 'scooter',
    'scoot': 'scooter',
    'scooter/moped': 'scooter',

    'box truck': 'truck',
    'tractor truck diesel': 'truck',

    'left scene': 'other',
    'liabitiy': 'other',
    'campe': 'other',
    'lawnmower': 'other',
}

vehicles['VEHICLE_TYPE'] = vehicles['VEHICLE_TYPE'].replace(vehicle_mapping)

# 6.6 group rare categories as 'other' (non logical / too specific)
value_counts = vehicles['VEHICLE_TYPE'].value_counts()
rare_types = value_counts[value_counts < 100].index  # threshold can be changed
vehicles['VEHICLE_TYPE'] = vehicles['VEHICLE_TYPE'].replace(rare_types, 'other')

# 6.7 final formatting (for dropdowns)
vehicles['VEHICLE_TYPE'] = vehicles['VEHICLE_TYPE'].str.title()

# -------------------------------------------------------------------
# 7) CLEAN VEHICLE_YEAR
# -------------------------------------------------------------------

vehicles['VEHICLE_YEAR'] = pd.to_numeric(vehicles['VEHICLE_YEAR'], errors='coerce')
vehicles['VEHICLE_YEAR'] = vehicles['VEHICLE_YEAR'].fillna(0).astype(int)
vehicles['VEHICLE_YEAR'] = vehicles['VEHICLE_YEAR'].mask(
    ~vehicles['VEHICLE_YEAR'].between(1900, 2025),
    0
)

# -------------------------------------------------------------------
# 8) DRIVER COLUMNS
#     - missing -> 'Unknown'
#     - 'nan' / '' -> 'Unknown'
# -------------------------------------------------------------------

driver_cols = [
    'DRIVER_SEX',
    'DRIVER_LICENSE_STATUS',
    'DRIVER_LICENSE_JURISDICTION'
]

for col in driver_cols:
    vehicles[col] = (
        vehicles[col]
        .fillna('Unknown')
        .astype(str)
        .str.strip()
    )
    vehicles[col] = vehicles[col].replace(
        ['nan', 'NaN', 'NONE', 'None', '', 'N/A', 'n/a'],
        'Unknown'
    )
    vehicles[col] = vehicles[col].str.title()

# small special case: driver sex â€” keep M/F as-is, map 'U' to 'Unknown'
vehicles['DRIVER_SEX'] = vehicles['DRIVER_SEX'].replace({'U': 'Unknown'})

# -------------------------------------------------------------------
# 9) STATE_REGISTRATION
# -------------------------------------------------------------------

vehicles['STATE_REGISTRATION'] = (
    vehicles['STATE_REGISTRATION']
    .fillna('Unknown')
    .astype(str)
    .str.strip()
)

vehicles['STATE_REGISTRATION'] = vehicles['STATE_REGISTRATION'].replace(
    ['nan', 'NaN', 'NONE', 'None', '', 'N/A', 'n/a'],
    'Unknown'
)

vehicles['STATE_REGISTRATION'] = vehicles['STATE_REGISTRATION'].str.upper()

# -------------------------------------------------------------------
# 10) DAMAGE COLUMNS
# -------------------------------------------------------------------

damage_cols = [
    'VEHICLE_DAMAGE',
    'VEHICLE_DAMAGE_1',
    'VEHICLE_DAMAGE_2',
    'VEHICLE_DAMAGE_3',
    'PUBLIC_PROPERTY_DAMAGE_TYPE'
]

for col in damage_cols:
    vehicles[col] = (
        vehicles[col]
        .fillna('Unknown')
        .astype(str)
        .str.strip()
    )
    vehicles[col] = vehicles[col].replace(
        ['nan', 'NaN', 'NONE', 'None', '', 'N/A', 'n/a'],
        'Unknown'
    )
    vehicles[col] = vehicles[col].str.title()

# PUBLIC_PROPERTY_DAMAGE itself is like a flag (Y/N/Unspecified)
vehicles['PUBLIC_PROPERTY_DAMAGE'] = (
    vehicles['PUBLIC_PROPERTY_DAMAGE']
    .fillna('Unknown')
    .astype(str)
    .str.strip()
)
vehicles['PUBLIC_PROPERTY_DAMAGE'] = vehicles['PUBLIC_PROPERTY_DAMAGE'].replace(
    ['nan', 'NaN', 'NONE', 'None', '', 'N/A', 'n/a'],
    'Unknown'
)

# -------------------------------------------------------------------
# 11) IMPACT COLUMNS
# -------------------------------------------------------------------

impact_cols = ['POINT_OF_IMPACT', 'TRAVEL_DIRECTION']

for col in impact_cols:
    vehicles[col] = (
        vehicles[col]
        .fillna('Unknown')
        .astype(str)
        .str.strip()
    )
    vehicles[col] = vehicles[col].replace(
        ['nan', 'NaN', 'NONE', 'None', '', 'N/A', 'n/a'],
        'Unknown'
    )
    vehicles[col] = vehicles[col].str.title()

# -------------------------------------------------------------------
# 12) CONTRIBUTING FACTORS
# -------------------------------------------------------------------

factor_cols = ['CONTRIBUTING_FACTOR_1', 'CONTRIBUTING_FACTOR_2']

for col in factor_cols:
    vehicles[col] = (
        vehicles[col]
        .fillna('Unspecified')
        .astype(str)
        .str.strip()
    )
    vehicles[col] = vehicles[col].replace(
        ['nan', 'NaN', 'NONE', 'None', '', 'N/A', 'n/a'],
        'Unspecified'
    )
    vehicles[col] = vehicles[col].str.title()

# -------------------------------------------------------------------
# 13) VEHICLE_OCCUPANTS
#     (numeric count: missing -> 0 is OK)
# -------------------------------------------------------------------

vehicles['VEHICLE_OCCUPANTS'] = vehicles['VEHICLE_OCCUPANTS'].fillna(0)

# -------------------------------------------------------------------
# 14) PRE_CRASH behavior
# -------------------------------------------------------------------

vehicles['PRE_CRASH'] = (
    vehicles['PRE_CRASH']
    .fillna('Unknown')
    .astype(str)
    .str.strip()
)
vehicles['PRE_CRASH'] = vehicles['PRE_CRASH'].replace(
    ['nan', 'NaN', 'NONE', 'None', '', 'N/A', 'n/a'],
    'Unknown'
)
vehicles['PRE_CRASH'] = vehicles['PRE_CRASH'].str.title()

# -------------------------------------------------------------------
# 15) Remove duplicate vehicle rows
# -------------------------------------------------------------------

vehicles.drop_duplicates(subset=['UNIQUE_ID'], keep='first', inplace=True)

vehicles.info()
vehicles.isna().sum().sort_values(ascending=False).head(12)

# Create vehicle_count table
vehicle_counts = (
    vehicles
    .groupby("COLLISION_ID")
    .size()
    .reset_index(name="VEHICLE_COUNT")
)

merged = crashes.merge(
    vehicles,
    on="COLLISION_ID",
    how="inner",
    suffixes=("_CRASH", "_VEHICLE")
)


print(merged.shape)
merged.head()

1. '''Dataset Downloading Strategy (Chunk-Based Loading)

The original NYC datasets are extremely large
(2+ million crashes and 3+ million vehicle records).
Loading them all at once would crash Colab.

To solve this, I used chunk-based downloading:

crashes_iter = pd.read_csv(crashes_url, chunksize=250000, low_memory=False)
vehicles_iter = pd.read_csv(vehicles_url, chunksize=250000, low_memory=False)

âœ” Why this is needed?

Colab RAM cannot load millions of rows at once.

Using chunks lets us stream the data gradually.

We can load only the amount needed for analysis (e.g., first 750k crashes).

I then combined only the first few chunks:

crashes = pd.concat([next(crashes_iter) for _ in range(3)], ignore_index=True)
vehicles_chunks = []
for chunk in vehicles_iter:
    vehicles_chunks.append(chunk)
vehicles = pd.concat(vehicles_chunks, ignore_index=True)


This ensures fast loading and avoids memory overflow.

ðŸ“Œ 2. Sampling Crashes Before Filtering Vehicles

The crash dataset is much larger than needed for Milestone 1.
So I sampled a clean, representative subset:

crashes = crashes.sample(300000, random_state=42).reset_index(drop=True)

âœ” Why sample crashes first?

Vehicles depend on crashes â†’ sampling crashes defines the "universe"

Ensures the sample still contains real COLLISION_IDs that exist in vehicles

Keeps dataset balanced across years, boroughs, factors, etc.

After sampling, I extracted the crash IDs:

sampled_ids = crashes['COLLISION_ID'].unique()

ðŸ“Œ 3. Aligning the Vehicle Dataset to the Sampled Crash IDs

The vehicle dataset contains multiple rows per crash
(one record per vehicle involved).
So to avoid unnecessary memory usage, I filtered vehicles during loading:

for chunk in vehicles_iter:
    filtered = chunk[chunk['COLLISION_ID'].isin(sampled_ids)]
    vehicles_list.append(filtered)

vehicles = pd.concat(vehicles_list, ignore_index=True)

âœ” Why filter vehicles?

We only need vehicles involved in the sampled crashes

Keeps memory low

Speeds up merging

Preserves the correct one-to-many structure

ðŸ“Œ 4. Understanding COLLISION_ID (Key for Integration)

Both datasets share the same key column:

COLLISION_ID
Dataset	Meaning
Crashes	One row per crash
Vehicles	One row per vehicle in the crash (1-5 vehicles or more)

This creates a 1-to-many relationship:

ONE crash  â†’  MANY vehicles


Example:

COLLISION_ID	Vehicles
123456	Car, SUV, Taxi â†’ 3 rows
789012	Motorcycle only â†’ 1 row

This is exactly why merging works.

ðŸ“Œ 5. Final Merge Step (Crash + Vehicle Data)

After cleaning both datasets, I merged them using:

merged = crashes.merge(vehicles, on='COLLISION_ID', how='inner')

âœ” Why inner join?

Because:

We sampled crashes first

Then filtered vehicles to match those crashes

Both datasets already aligned

inner keeps only collisions present in both (which is intended)

This results in a fully integrated dataset, with:

crash-level information

vehicle-level information

datetime components

location data

injury & fatality details

driver and vehicle attributes

ðŸ“Œ 6. Final Integrated Dataset Overview

After merging:

Every row represents one vehicle involved in one crash

Crash attributes repeat for all vehicles in that crash

Vehicle attributes differ per row

Example row structure:

COLLISION_ID	BOROUGH	CRASH_DATETIME	VEHICLE_TYPE	DRIVER_SEX	â€¦

This merged structure is perfect for:

interactive dashboards

filtering by crash or vehicle features

map visualizations

research questions

year/borough/vehicle-type analysis '''

import pandas as pd
import numpy as np

cleaned = merged.copy()

# ============================================================
# 1) FIX MERGE-RELATED MISSING VALUES (SMARTER, NOT BLIND)
# ============================================================

fill_unknown = [
    'VEHICLE_TYPE', 'VEHICLE_MAKE', 'VEHICLE_MODEL',
    'STATE_REGISTRATION', 'POINT_OF_IMPACT', 'TRAVEL_DIRECTION',
    'VEHICLE_DAMAGE', 'VEHICLE_DAMAGE_1', 'VEHICLE_DAMAGE_2',
    'VEHICLE_DAMAGE_3', 'PUBLIC_PROPERTY_DAMAGE_TYPE',
    'PRE_CRASH'
]

for col in fill_unknown:
    if col in cleaned.columns:
        cleaned[col] = cleaned[col].fillna('Unknown')

fill_unspecified = ['CONTRIBUTING_FACTOR_1', 'CONTRIBUTING_FACTOR_2']
for col in fill_unspecified:
    if col in cleaned.columns:
        cleaned[col] = cleaned[col].fillna('Unspecified')

numeric_fill = ['VEHICLE_OCCUPANTS', 'VEHICLE_YEAR']
for col in numeric_fill:
    if col in cleaned.columns:
        cleaned[col] = cleaned[col].fillna(0)


# ============================================================
# 2) DROP REDUNDANT & BROKEN COLUMNS
# ============================================================

drop_cols = [
    'YEAR_VEHICLE', 'MONTH_VEHICLE', 'HOUR_VEHICLE',
    'DAY_OF_WEEK_VEHICLE'
]

cleaned = cleaned.drop(columns=[c for c in drop_cols if c in cleaned.columns], errors='ignore')


# ============================================================
# 3) ZIP CODE CLEANING
# ============================================================

cleaned['ZIP CODE'] = (
    cleaned['ZIP CODE']
    .astype(str)
    .str.replace('.0', '', regex=False)
)

# empty, 0, 00000 â†’ Unknown
cleaned['ZIP CODE'] = cleaned['ZIP CODE'].replace(
    ['nan', 'None', 'NONE', '', '0', '00000'], 'Unknown'
)


# ============================================================
# 4) FIX COORDINATES (VERY IMPORTANT)
# ============================================================

# convert to numeric
cleaned['LATITUDE'] = pd.to_numeric(cleaned['LATITUDE'], errors='coerce')
cleaned['LONGITUDE'] = pd.to_numeric(cleaned['LONGITUDE'], errors='coerce')

# Invalid NY ranges â†’ set to NaN
cleaned.loc[(cleaned['LATITUDE'] < 40) | (cleaned['LATITUDE'] > 41), 'LATITUDE'] = np.nan
cleaned.loc[(cleaned['LONGITUDE'] > -72) | (cleaned['LONGITUDE'] < -75), 'LONGITUDE'] = np.nan


# ============================================================
# 5) DO NOT TITLE-CASE EVERYTHING (BREAKS VALUES!)
# Only fix whitespace safely.
# ============================================================

for col in cleaned.select_dtypes(include=['object']).columns:
    cleaned[col] = cleaned[col].astype(str).str.strip()


# ============================================================
# 6) FINAL CRASH_DATETIME COMBINATION
# ============================================================

cleaned['CRASH_DATETIME'] = pd.to_datetime(cleaned['CRASH_DATETIME_CRASH'], errors='coerce')

cleaned['CRASH_DATETIME'] = cleaned['CRASH_DATETIME'].fillna(
    pd.to_datetime(cleaned['CRASH_DATETIME_VEHICLE'], errors='coerce')
)

cleaned['YEAR'] = cleaned['CRASH_DATETIME'].dt.year
cleaned['MONTH'] = cleaned['CRASH_DATETIME'].dt.month
cleaned['HOUR'] = cleaned['CRASH_DATETIME'].dt.hour
cleaned['DAY_OF_WEEK'] = cleaned['CRASH_DATETIME'].dt.day_name()


# ============================================================
# 7) ADD VEHICLE COUNT PER CRASH
# ============================================================

cleaned['VEHICLE_COUNT'] = cleaned.groupby('COLLISION_ID')['COLLISION_ID'].transform('count')


# ============================================================
# 8) FINAL REPORT
# ============================================================

print("POST-INTEGRATION CLEANING COMPLETE")
print("Shape before:", merged.shape)
print("Shape after :", cleaned.shape)

print("\nMissing:\n", cleaned.isna().sum().sort_values(ascending=False).head(20))

# ================================
# POST-INTEGRATION EDA â€” MERGED
# ================================

# ======================================
# 1. Crashes by Borough (After Merge)
# ======================================
plt.figure(figsize=(8,5))
sns.countplot(y='BOROUGH', data=cleaned)
plt.title("Crashes by Borough (After Integration)")
plt.show()

# ======================================
# 2. Top Vehicle Types in Crashes
# ======================================
top_veh = cleaned['VEHICLE_TYPE'].value_counts().head(10)

plt.figure(figsize=(10,5))
sns.barplot(x=top_veh.values, y=top_veh.index)
plt.title("Top Vehicle Types Involved in Crashes")
plt.show()

# ======================================
# 3. Combined Injury & Fatality Distribution
# ======================================
plt.figure(figsize=(12,5))
sns.boxplot(data=cleaned[['NUMBER OF PERSONS INJURED','NUMBER OF PERSONS KILLED']])
plt.title("Injuries & Fatalities Distribution (Merged Data)")
plt.show()

# ======================================
# 4. Correlation Heatmap
# ======================================
numeric_cols = cleaned.select_dtypes(include=['int','float']).corr()

plt.figure(figsize=(12,6))
sns.heatmap(numeric_cols, cmap="coolwarm")
plt.title("Correlation Heatmap â€” Merged Dataset")
plt.show()

# ======================================
# 5. Crash Severity by Vehicle Type
# ======================================
severity_by_type = cleaned.groupby('VEHICLE_TYPE')['NUMBER OF PERSONS INJURED'].sum().sort_values(ascending=False).head(10)

plt.figure(figsize=(10,6))
sns.barplot(x=severity_by_type.values, y=severity_by_type.index)
plt.title("Total Injuries by Vehicle Type")
plt.xlabel("Total Injuries")
plt.ylabel("Vehicle Type")
plt.show()

# ======================================
# 6. Interactive Crash Location Map
# ======================================
map_df = cleaned.dropna(subset=['LATITUDE', 'LONGITUDE'])

fig = px.scatter_mapbox(
    map_df,
    lat='LATITUDE',
    lon='LONGITUDE',
    hover_name='BOROUGH',
    hover_data=['CRASH_DATE','VEHICLE_TYPE'],
    zoom=10,
    height=650
)
fig.update_layout(mapbox_style="carto-positron")
fig.update_layout(
    mapbox=dict(
        center={"lat": 40.7128, "lon": -74.0060},
        zoom=9
    )
)
fig.show()

print("======================================")
print(" BASIC SHAPE & INFO ")
print("======================================")

print("Shape:", cleaned.shape)
display(cleaned.info())


print("\n======================================")
print(" MISSING VALUES CHECK ")
print("======================================")

missing = cleaned.isna().sum().sort_values(ascending=False)
display(missing.head(20))


print("\n======================================")
print(" DUPLICATE COLLISION_ID CHECK ")
print("======================================")

dup_check = cleaned['COLLISION_ID'].duplicated().sum()
print("Number of duplicate COLLISION_ID entries:", dup_check)


print("\n======================================")
print(" DAY OF WEEK DISTRIBUTION ")
print("======================================")

display(cleaned['DAY_OF_WEEK'].value_counts())


print("\n======================================")
print(" YEAR / MONTH DISTRIBUTION ")
print("======================================")

display(cleaned['YEAR'].value_counts().sort_index())
display(cleaned['MONTH'].value_counts().sort_index())


print("\n======================================")
print(" LAT/LON CHECK (VALID COORDINATES) ")
print("======================================")

print("Latitude range:", cleaned['LATITUDE'].min(), "to", cleaned['LATITUDE'].max())
print("Longitude range:", cleaned['LONGITUDE'].min(), "to", cleaned['LONGITUDE'].max())

print("\nCoordinates count after removing NaN:")
coords_ok = cleaned.dropna(subset=['LATITUDE', 'LONGITUDE'])
print(coords_ok.shape)


print("\n======================================")
print(" CHECK: 1 crash â†’ many vehicles ")
print("======================================")

vehicle_per_crash = cleaned.groupby('COLLISION_ID').size()
display(vehicle_per_crash.describe())  # average, min, max


print("\n======================================")
print(" RANDOM SAMPLE ROWS ")
print("======================================")

display(cleaned.sample(5))